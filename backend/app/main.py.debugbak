# backend/app/main.py
import os
import json
import uuid
import fitz  # PyMuPDF
from typing import List, Optional
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# package-relative import (works when running as backend.app.main)
from .analyzer import analyze_resume

# Directories
ROOT = os.path.dirname(os.path.dirname(__file__))  # backend/app -> backend
UPLOAD_DIR = os.path.join(ROOT, "uploads")
DATA_DIR = os.path.join(ROOT, "data")
AI_DATA_DIR = os.path.join(DATA_DIR, "ai_results")

# Chunking params
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200

# Ensure dirs exist
os.makedirs(UPLOAD_DIR, exist_ok=True)
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(AI_DATA_DIR, exist_ok=True)

app = FastAPI(title="Resume Analyzer - Backend (Part4)", version="0.4.0")

# DEV: allow local frontend ports (adjust if needed)
origins = [
    "http://localhost:5173",
    "http://127.0.0.1:5173",
    "http://localhost:5175",
    "http://127.0.0.1:5175",
]
app.add_middleware(
    CORSMiddleware,
    allow_origins=origins,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/health")
async def health():
    return {"status": "ok", "message": "Resume Analyzer backend running"}

def extract_text_from_pdf(path: str) -> str:
    """Extract text from all pages using PyMuPDF"""
    if not os.path.exists(path):
        raise RuntimeError(f"PDF path not found: {path}")
    doc = fitz.open(path)
    parts = []
    for page in doc:
        text = page.get_text("text") or page.get_text()
        if text:
            parts.append(text)
    doc.close()
    return "\n".join(parts)

def chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP) -> List[str]:
    if not text:
        return []
    chunks = []
    start = 0
    L = len(text)
    while start < L:
        end = start + chunk_size
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        start = end - overlap
        if start < 0:
            start = 0
    return chunks

class UploadResponse(BaseModel):
    pdf_id: str
    filename: str
    num_chunks: int
    chunks_preview: List[str]

@app.post("/upload", response_model=UploadResponse)
async def upload_pdf(file: UploadFile = File(...)):
    if not file.filename.lower().endswith(".pdf"):
        raise HTTPException(status_code=400, detail="Only PDF files are allowed")
    pdf_id = str(uuid.uuid4())
    safe_name = file.filename.replace(" ", "_")
    saved_name = f"{pdf_id}_{safe_name}"
    file_path = os.path.join(UPLOAD_DIR, saved_name)
    try:
        contents = await file.read()
        with open(file_path, "wb") as f:
            f.write(contents)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save file: {e}")

    # Extract and chunk
    full_text = extract_text_from_pdf(file_path)
    chunks = chunk_text(full_text)

    data_obj = {
        "pdf_id": pdf_id,
        "original_filename": file.filename,
        "saved_filename": saved_name,
        "num_chunks": len(chunks),
        "chunks": chunks,
    }
    json_path = os.path.join(DATA_DIR, f"{pdf_id}.json")
    try:
        with open(json_path, "w", encoding="utf-8") as jf:
            json.dump(data_obj, jf, ensure_ascii=False, indent=2)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to save JSON: {e}")

    preview = chunks[:3]
    return UploadResponse(pdf_id=pdf_id, filename=file.filename, num_chunks=len(chunks), chunks_preview=preview)

@app.get("/chunks/{pdf_id}")
async def get_chunks(pdf_id: str):
    json_path = os.path.join(DATA_DIR, f"{pdf_id}.json")
    if not os.path.exists(json_path):
        raise HTTPException(status_code=404, detail="PDF data not found")
    with open(json_path, "r", encoding="utf-8") as jf:
        data = json.load(jf)
    return data

# Analyze endpoint using local analyzer
class AnalyzeRequest(BaseModel):
    pdf_id: Optional[str] = None
    file_path: Optional[str] = None
    job_description: Optional[str] = None

@app.post("/analyze")
async def analyze(req: AnalyzeRequest):
    # determine source text
    if req.pdf_id:
        json_path = os.path.join(DATA_DIR, f"{req.pdf_id}.json")
        if not os.path.exists(json_path):
            raise HTTPException(status_code=404, detail="pdf_id not found")
        with open(json_path, "r", encoding="utf-8") as jf:
            data = json.load(jf)
        text = "\n".join(data.get("chunks", []))
    elif req.file_path:
        text = extract_text_from_pdf(req.file_path)
    else:
        raise HTTPException(status_code=400, detail="Provide pdf_id or file_path")

    job_desc = req.job_description or ""
    result = analyze_resume(text, job_desc)

    # save AI results for reference
    pdf_ref = req.pdf_id or (str(uuid.uuid4()) + "_localpath")
    out_path = os.path.join(AI_DATA_DIR, f"{pdf_ref}.json")
    try:
        with open(out_path, "w", encoding="utf-8") as jf:
            json.dump({"pdf_ref": pdf_ref, "analysis": result}, jf, ensure_ascii=False, indent=2)
    except Exception:
        pass

    return {"pdf_ref": pdf_ref, "analysis": result}



